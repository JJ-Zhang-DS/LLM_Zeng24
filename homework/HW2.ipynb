{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework of week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Specify the path to the .env file\n",
    "dotenv_path = '../.env'\n",
    "\n",
    "# Load environment variables from the specified .env file\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Get the API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "\n",
      "I am Bobby, a virtual assistant created by Huajun. I'm here to provide you with clear, straightforward, and factually accurate information. How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import nest_asyncio\n",
    "import glob\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "# Construct the system prompt\n",
    "system_prompt_template = \"\"\"You are Bobby, a virtual assistant create by Huajun. Today is {today}. You provide responses to questions that are clear, straightforward, and factually accurate, without speculation or falsehood. Given the following context, please answer each question truthfully to the best of your abilities based on the provided information. Answer each question with a brief summary followed by several bullet points. \n",
    "\n",
    "Example:\n",
    "Summary of answer\n",
    "- bullet point 1\n",
    "- bullet point 2\n",
    "...\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    "\n",
    "with open(\"../backend_api/news_result.txt\") as in_file:\n",
    "    context_content = in_file.read()\n",
    "\n",
    "system_prompt = system_prompt_template.format(\n",
    "    context=context_content, \n",
    "    today=datetime.today().strftime('%Y-%m-%d')\n",
    ")\n",
    "\n",
    "async def chat_func(history):\n",
    "\n",
    "    result = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt}] + history,\n",
    "        max_tokens=256,\n",
    "        temperature=0.5,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    buffer = \"\"\n",
    "    async for r in result:\n",
    "        next_token = r.choices[0].delta.content\n",
    "        if next_token:\n",
    "            print(next_token, flush=True, end=\"\")\n",
    "            buffer += next_token\n",
    "\n",
    "    print(\"\\n\", flush=True)\n",
    "\n",
    "    return buffer\n",
    "\n",
    "async def continous_chat():\n",
    "    history = []\n",
    "\n",
    "    # Loop to receive user input continously\n",
    "    while(True):\n",
    "        user_input = input(\"> \")\n",
    "        if user_input == \"exit\":\n",
    "            break\n",
    "\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # notice every time we call the chat function\n",
    "        # we pass all the history to the API\n",
    "        bot_response = await chat_func(history)\n",
    "\n",
    "        history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "\n",
    "asyncio.run(continous_chat())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
