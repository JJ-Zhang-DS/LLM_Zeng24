{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain backend -- lecture 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 00 langchain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/junjiezhang/Documents/GitHub/LLM_Zeng24/backend_langchain'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# get the current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "\n",
    "# Loader for multiple text files\n",
    "loader = DirectoryLoader('./', glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "# Load all text files in the current directory\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "# Split the loaded documents into chunks of 1000 characters with 200 characters overlap\n",
    "\n",
    "# Create embeddings for each chunk\n",
    "vectorstore = Chroma.from_documents(chunks, OpenAIEmbeddings())\n",
    "# Generate embeddings for each chunk using OpenAI embeddings and store them in a vector database\n",
    "\n",
    "# Define a retriever from the vector DB\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "# Create a retriever that will fetch the top 6 most similar chunks based on similarity search\n",
    "\n",
    "# Get a prompt from LangChain hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# Retrieve a predefined prompt from the LangChain hub\n",
    "\n",
    "# Define the LLM object using OpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "# Initialize the language model (LLM) with OpenAI's GPT-3.5-turbo with a temperature of 0 for deterministic responses\n",
    "\n",
    "# The function to combine multiple document into one\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "# Define a function to concatenate the content of multiple documents into a single string\n",
    "\n",
    "# Define whole chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# Define the Retrieval-Augmented Generation (RAG) chain:\n",
    "# 1. Retrieve relevant document chunks and format them\n",
    "# 2. Combine the formatted context with the question\n",
    "# 3. Pass the combined input to the language model\n",
    "# 4. Parse the output string\n",
    "\n",
    "result = rag_chain.invoke(\"What is attention?\")\n",
    "print(result)\n",
    "# Invoke the RAG chain with the question \"What is attention?\" and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Loader for multiple text files\n",
    "loader = DirectoryLoader('/Users/junjiezhang/Documents/GitHub/LLM_Zeng24/backend_langchain', glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings for each chunk\n",
    "vectorstore = Chroma.from_documents(chunks, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "\n",
    "\n",
    "# Define a retriever from the vector DB\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "# Get a prompt from LangChain hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM object using OpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to combine multiple document into one\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define whole chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention is a mechanism that maps a query and key-value pairs to an output\n",
      "using vectors. The output is a weighted sum of values based on the compatibility\n",
      "of the query with the corresponding key. Self-attention is an example of\n",
      "attention used in various tasks like reading comprehension and summarization.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "result = rag_chain.invoke(\"What is attention?\")\n",
    "wrapped_result = textwrap.fill(result, width=80)\n",
    "print(wrapped_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 01: chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "llm1 = ChatOpenAI()\n",
    "llm2 = ChatAnthropic(model_name=\"claude-2.0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result from OpenAI: 1. SoleStride Footwear\n",
      "2. StepUp Shoes\n",
      "3. Peak Performance Footwear\n",
      "4. StrideTrend Shoes\n",
      "5. Trek & Tread Shoes\n",
      "6. UrbanWalk Footwear\n",
      "7. SwiftStride Shoes\n",
      "8. BoldStep Footwear\n",
      "9. VentureWalk Shoes\n",
      "10. SteadyStride Footwear\n"
     ]
    }
   ],
   "source": [
    "# Define the messages for the conversation\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"Suggest some good names for a shoe company.\"),\n",
    "]\n",
    "# The SystemMessage sets the context for the assistant\n",
    "# The HumanMessage asks the assistant to suggest names for a shoe company\n",
    "\n",
    "# Invoke the first language model (llm1) with the messages\n",
    "result1 = llm1.invoke(messages)\n",
    "print(\"Result from OpenAI:\", result1.content)\n",
    "# Print the result from the OpenAI model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key for Anthropic\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/60/j13rv8sj6h7_s2bchmfjrwbm0000gn/T/ipykernel_34909/2955471340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Invoke the second language model (llm2) with the messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Result from Anthropic:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Print the result from the Anthropic model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         return cast(\n\u001b[1;32m    285\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    785\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     async def agenerate_prompt(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_anthropic/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    899\u001b[0m             )\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    902\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         )\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     def patch(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         return self._process_response(\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "# Set the API key for the Anthropic model\n",
    "llm2 = ChatAnthropic(model_name=\"claude-2.0\", api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "# Invoke the second language model (llm2) with the messages\n",
    "result2 = llm2.invoke(messages)\n",
    "print(\"Result from Anthropic:\", result2.content)\n",
    "# Print the result from the Anthropic model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming result from OpenAI:\n",
      "1. Soles and Strides\n",
      "2. Footwear Fusion\n",
      "3. StepStar\n",
      "4. WalkWise\n",
      "5. HappyFeet Co.\n",
      "6. UrbanKick\n",
      "7. SoleHaven\n",
      "8. StrideStyle\n",
      "9. Footworks Studio\n",
      "10. Steppin' Up Shoes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream results from the first language model (llm1)\n",
    "print(\"Streaming result from OpenAI:\")\n",
    "for chunk in llm1.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "# Print each chunk of the response from the OpenAI model in real-time\n",
    "\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming result from Anthropic:\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/60/j13rv8sj6h7_s2bchmfjrwbm0000gn/T/ipykernel_34909/2220640109.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Stream results from the second language model (llm2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Streaming result from Anthropic:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mllm2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print each chunk of the response from the Anthropic model in real-time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m                     ),\n\u001b[1;32m    419\u001b[0m                 )\n\u001b[0;32m--> 420\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                         \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"run-{run_manager.run_id}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_anthropic/chat_models.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, messages, stop, run_manager, stream_usage, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_request_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0mcoerce_content_to_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_tools_in_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    899\u001b[0m             )\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    902\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         )\n\u001b[0;32m-> 1282\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     def patch(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         return self._process_response(\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Anthropic API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
     ]
    }
   ],
   "source": [
    "# Stream results from the second language model (llm2)\n",
    "print(\"Streaming result from Anthropic:\")\n",
    "for chunk in llm2.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "# Print each chunk of the response from the Anthropic model in real-time\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 2: chat with promote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful assistant that translates English to French.\n",
      "Human: I love programming.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary classes from langchain_openai and langchain.schema\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Define the system message template for setting the context\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "\n",
    "# Define the human message template for the input text\n",
    "human_template = \"{text}\"\n",
    "\n",
    "# Create the final prompt template using the system and human templates\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),  # System message placeholder\n",
    "    (\"human\", human_template),    # Human message placeholder\n",
    "])\n",
    "\n",
    "# Uncomment the following lines to see the formatted prompt\n",
    "print(final_prompt.format(\n",
    "    input_language=\"English\", \n",
    "    output_language=\"French\", \n",
    "    text=\"I love programming.\",\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The waterfall cascades down three thousand feet, resembling the Milky Way plunging from the ninth heaven.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chat model\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "# Create a chain by combining the final prompt and the chat model\n",
    "chain = final_prompt | chat_model\n",
    "\n",
    "# Invoke the chain with specific input parameters\n",
    "result = chain.invoke(dict(\n",
    "    input_language=\"Chinese\", \n",
    "    output_language=\"English\", \n",
    "    text=\"飞流直下三千尺，疑是银河落九天。\",\n",
    "))\n",
    "\n",
    "# Print the result content\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 3: chat with few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好 (nǐ hǎo)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Define examples for few-shot learning\n",
    "examples = [\n",
    "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2+3\", \"output\": \"5\"},\n",
    "]\n",
    "\n",
    "# Create a prompt template for the examples\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),  # Human message placeholder\n",
    "        (\"ai\", \"{output}\"),    # AI response placeholder\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a few-shot prompt template using the examples\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Create the final prompt template\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a wondrous wizard of math.\"),  # System message setting the context\n",
    "        few_shot_prompt,  # Include the few-shot examples\n",
    "        (\"human\", \"{input}\"),  # Placeholder for the human input\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the language model with a temperature setting\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create a chain by combining the final prompt and the language model\n",
    "chain = final_prompt | llm\n",
    "\n",
    "# Invoke the chain with a specific input\n",
    "result = chain.invoke({\"input\": \"how to say hello in chinese?\"})\n",
    "\n",
    "# Print the result content\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 4: output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes from langchain_openai, langchain.prompts, and langchain.schema\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'blue', 'green', 'yellow', 'purple']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define a custom output parser class to parse the output into a comma-separated list\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "# Define the system message template for setting the context\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "# Define the human message template for the input text\n",
    "human_template = \"{text}\"\n",
    "\n",
    "# Create the final prompt template using the system and human templates\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),  # System message placeholder\n",
    "    (\"human\", human_template),  # Human message placeholder\n",
    "])\n",
    "\n",
    "# Create a chain by combining the final prompt, the chat model, and the custom output parser\n",
    "chain = final_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n",
    "\n",
    "# Invoke the chain with a specific input\n",
    "result = chain.invoke({\"text\": \"colors\"})\n",
    "\n",
    "# Print the parsed result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 05 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harrison worked at Kensho.\n"
     ]
    }
   ],
   "source": [
    "# Define a list of text data\n",
    "text_list = [\n",
    "    \"Johnson worked at MS\",\n",
    "    \"Harry worked at Meta\",\n",
    "    \"Hua worked at Alibaba\",\n",
    "    \"Harrison worked at Kensho\",\n",
    "]\n",
    "\n",
    "# Create a vector store from the text data using OpenAI embeddings\n",
    "vectorstore = FAISS.from_texts(text_list, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define the prompt template for the model\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize the chat model\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Create a chain by combining the retriever, prompt, model, and output parser\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}  # Pass the retriever and question\n",
    "    | prompt  # Apply the prompt template\n",
    "    | model  # Use the chat model\n",
    "    | StrOutputParser()  # Parse the output as a string\n",
    ")\n",
    "\n",
    "# Invoke the chain with a specific question\n",
    "result = chain.invoke(\"where did Harrison work?\")\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 6 chat with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat model\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Create the prompt template for the conversation\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        # System message to set the context for the chatbot\n",
    "        SystemMessagePromptTemplate.from_template(\"You are a nice chatbot having a conversation with a human.\"),\n",
    "        # Placeholder for the chat history to maintain the conversation context\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        # Template for the human message input\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a memory object to store the conversation history\n",
    "# `return_messages=True` ensures that the memory returns messages to fit into the MessagesPlaceholder\n",
    "# `memory_key=\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create a conversation chain by combining the chat model, prompt, and memory\n",
    "conversation = LLMChain(\n",
    "    llm=llm,  # The chat model\n",
    "    prompt=prompt,  # The prompt template\n",
    "    verbose=True,  # Enable verbose mode to see detailed logs\n",
    "    memory=memory  # The memory object to store conversation history\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: Hi, my name is Huajun.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello Huajun! It's nice to meet you. How are you today?\n"
     ]
    }
   ],
   "source": [
    "# Invoke the conversation chain with a specific question\n",
    "# The `chat_history` gets populated by the memory automatically\n",
    "result = conversation.invoke({\"question\": \"Hi, my name is Huajun.\"})\n",
    "print(result[\"text\"])  # Print the chatbot's response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: Hi, my name is Huajun.\n",
      "AI: Hello Huajun! It's nice to meet you. How are you today?\n",
      "Human: Write a poem for me.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Of course, Huajun! Here's a poem just for you:\n",
      "\n",
      "In a world of beauty, so vast and wide,\n",
      "Huajun stands with joy and pride.\n",
      "With a heart that's kind and true,\n",
      "Spreading love in all you do.\n",
      "\n",
      "May your days be filled with light,\n",
      "Guiding you through the darkest night.\n",
      "So let this poem be a token,\n",
      "Of the friendship that we've awoken.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the conversation chain with another question\n",
    "result = conversation.invoke({\"question\": \"Write a poem for me.\"})\n",
    "print(result[\"text\"])  # Print the chatbot's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 7 chat with memory rag continous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings\n",
      "Creating chains\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading documents\")\n",
    "# Load documents from the local disk using a directory loader\n",
    "loader = DirectoryLoader('./', glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents into smaller chunks for better processing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(\"Creating embeddings\")\n",
    "\n",
    "# Create embeddings for each chunk using OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "\n",
    "# Create a retriever from the vector store to fetch relevant chunks\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"Creating chains\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "computing:\n",
      "\n",
      "I. Introduction, his life and background\n",
      "\n",
      "A: His early life\n",
      "\n",
      "B:\n",
      "\n",
      "rewrite Covert my resume into a profile overview.\n",
      "\n",
      "{resume}\n",
      "\n",
      "Profile overview:\n",
      "\n",
      "rewrite Rephrase this for me: \"I can’t seem to find out how to work this darn thing.\"\n",
      "\n",
      "Alternate phrasing: \"\n",
      "\n",
      "rewrite Original: She no go to sleep.\n",
      "\n",
      "Standard American English: She didn’t go to sleep\n",
      "\n",
      "Original: It real bad for I to make do of this.\n",
      "\n",
      "Standard American English:\n",
      "\n",
      "chat The following is a conversation with an AI assistant. The assistant is helpful,\n",
      "\n",
      "creative, clever, and very friendly.\n",
      "\n",
      "Human: Hello, who are you?\n",
      "\n",
      "AI: I am an AI created by OpenAI. How can I help you today?\n",
      "\n",
      "Human: I’m feeling kind of down today.\n",
      "\n",
      "AI:\n",
      "\n",
      "Continued on next page\n",
      "\n",
      "31\n",
      "\n",
      "Use Case Example\n",
      "\n",
      "chat This is a conversation with Steven. Steven likes to watch Netflix and hasn’t left\n",
      "\n",
      "his home in 2 weeks.\n",
      "\n",
      "John: Hey man what’s up?\n",
      "\n",
      "Steven: Exactly the same thing as yesterday. you know.\n",
      "\n",
      "John: So we’re going to go see a movie on Thursday, want to come?\n",
      "\n",
      "computing:\n",
      "\n",
      "I. Introduction, his life and background\n",
      "\n",
      "A: His early life\n",
      "\n",
      "B:\n",
      "\n",
      "rewrite Covert my resume into a profile overview.\n",
      "\n",
      "{resume}\n",
      "\n",
      "Profile overview:\n",
      "\n",
      "rewrite Rephrase this for me: \"I can’t seem to find out how to work this darn thing.\"\n",
      "\n",
      "Alternate phrasing: \"\n",
      "\n",
      "rewrite Original: She no go to sleep.\n",
      "\n",
      "Standard American English: She didn’t go to sleep\n",
      "\n",
      "Original: It real bad for I to make do of this.\n",
      "\n",
      "Standard American English:\n",
      "\n",
      "chat The following is a conversation with an AI assistant. The assistant is helpful,\n",
      "\n",
      "creative, clever, and very friendly.\n",
      "\n",
      "Human: Hello, who are you?\n",
      "\n",
      "AI: I am an AI created by OpenAI. How can I help you today?\n",
      "\n",
      "Human: I’m feeling kind of down today.\n",
      "\n",
      "AI:\n",
      "\n",
      "Continued on next page\n",
      "\n",
      "31\n",
      "\n",
      "Use Case Example\n",
      "\n",
      "chat This is a conversation with Steven. Steven likes to watch Netflix and hasn’t left\n",
      "\n",
      "his home in 2 weeks.\n",
      "\n",
      "John: Hey man what’s up?\n",
      "\n",
      "Steven: Exactly the same thing as yesterday. you know.\n",
      "\n",
      "John: So we’re going to go see a movie on Thursday, want to come?\n",
      "\n",
      "computing:\n",
      "\n",
      "I. Introduction, his life and background\n",
      "\n",
      "A: His early life\n",
      "\n",
      "B:\n",
      "\n",
      "rewrite Covert my resume into a profile overview.\n",
      "\n",
      "{resume}\n",
      "\n",
      "Profile overview:\n",
      "\n",
      "rewrite Rephrase this for me: \"I can’t seem to find out how to work this darn thing.\"\n",
      "\n",
      "Alternate phrasing: \"\n",
      "\n",
      "rewrite Original: She no go to sleep.\n",
      "\n",
      "Standard American English: She didn’t go to sleep\n",
      "\n",
      "Original: It real bad for I to make do of this.\n",
      "\n",
      "Standard American English:\n",
      "\n",
      "chat The following is a conversation with an AI assistant. The assistant is helpful,\n",
      "\n",
      "creative, clever, and very friendly.\n",
      "\n",
      "Human: Hello, who are you?\n",
      "\n",
      "AI: I am an AI created by OpenAI. How can I help you today?\n",
      "\n",
      "Human: I’m feeling kind of down today.\n",
      "\n",
      "AI:\n",
      "\n",
      "Continued on next page\n",
      "\n",
      "31\n",
      "\n",
      "Use Case Example\n",
      "\n",
      "chat This is a conversation with Steven. Steven likes to watch Netflix and hasn’t left\n",
      "\n",
      "his home in 2 weeks.\n",
      "\n",
      "John: Hey man what’s up?\n",
      "\n",
      "Steven: Exactly the same thing as yesterday. you know.\n",
      "\n",
      "John: So we’re going to go see a movie on Thursday, want to come?\n",
      "\n",
      "computing:\n",
      "\n",
      "I. Introduction, his life and background\n",
      "\n",
      "A: His early life\n",
      "\n",
      "B:\n",
      "\n",
      "rewrite Covert my resume into a profile overview.\n",
      "\n",
      "{resume}\n",
      "\n",
      "Profile overview:\n",
      "\n",
      "rewrite Rephrase this for me: \"I can’t seem to find out how to work this darn thing.\"\n",
      "\n",
      "Alternate phrasing: \"\n",
      "\n",
      "rewrite Original: She no go to sleep.\n",
      "\n",
      "Standard American English: She didn’t go to sleep\n",
      "\n",
      "Original: It real bad for I to make do of this.\n",
      "\n",
      "Standard American English:\n",
      "\n",
      "chat The following is a conversation with an AI assistant. The assistant is helpful,\n",
      "\n",
      "creative, clever, and very friendly.\n",
      "\n",
      "Human: Hello, who are you?\n",
      "\n",
      "AI: I am an AI created by OpenAI. How can I help you today?\n",
      "\n",
      "Human: I’m feeling kind of down today.\n",
      "\n",
      "AI:\n",
      "\n",
      "Continued on next page\n",
      "\n",
      "31\n",
      "\n",
      "Use Case Example\n",
      "\n",
      "chat This is a conversation with Steven. Steven likes to watch Netflix and hasn’t left\n",
      "\n",
      "his home in 2 weeks.\n",
      "\n",
      "John: Hey man what’s up?\n",
      "\n",
      "Steven: Exactly the same thing as yesterday. you know.\n",
      "\n",
      "John: So we’re going to go see a movie on Thursday, want to come?\n",
      "Human: hello\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: hello\n",
      "Assistant: Hello! How can I assist you today?\n",
      "Follow Up Input: write a poem for me\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Must look on blue and green\n",
      "\n",
      "And never let my eye forget When our own breath gonne faint and fade\n",
      "\n",
      "That color is my friend Up the long stairway of the dead\n",
      "\n",
      "And purple must surround me too We shall remember well\n",
      "\n",
      "The blindness of the light, the whiteness\n",
      "\n",
      "The yellow of the sun is no more Of that white land. The footsteps, and\n",
      "\n",
      "Intrusive than the bluish snow The grasses in that light, and in the shade\n",
      "\n",
      "That falls on all of us. I must have The menacing life.\n",
      "\n",
      "Grey thoughts and blue thoughts walk with me\n",
      "\n",
      "If I am to go away at all. We played, too, once, in the warmest rooms.\n",
      "\n",
      "To one content, there is one content\n",
      "\n",
      "-------- Generated Poem 2 -------- For all mankind. The forest glades\n",
      "\n",
      "Are all the more for being in fable,\n",
      "\n",
      "There is a way in the world. And wood and town in legend shadowedThat way a man may live in a small town Once more to return. The one content\n",
      "\n",
      "His lifetime and see no man come, Has in it every happiness, more brave\n",
      "\n",
      "Must look on blue and green\n",
      "\n",
      "And never let my eye forget When our own breath gonne faint and fade\n",
      "\n",
      "That color is my friend Up the long stairway of the dead\n",
      "\n",
      "And purple must surround me too We shall remember well\n",
      "\n",
      "The blindness of the light, the whiteness\n",
      "\n",
      "The yellow of the sun is no more Of that white land. The footsteps, and\n",
      "\n",
      "Intrusive than the bluish snow The grasses in that light, and in the shade\n",
      "\n",
      "That falls on all of us. I must have The menacing life.\n",
      "\n",
      "Grey thoughts and blue thoughts walk with me\n",
      "\n",
      "If I am to go away at all. We played, too, once, in the warmest rooms.\n",
      "\n",
      "To one content, there is one content\n",
      "\n",
      "-------- Generated Poem 2 -------- For all mankind. The forest glades\n",
      "\n",
      "Are all the more for being in fable,\n",
      "\n",
      "There is a way in the world. And wood and town in legend shadowedThat way a man may live in a small town Once more to return. The one content\n",
      "\n",
      "His lifetime and see no man come, Has in it every happiness, more brave\n",
      "\n",
      "Must look on blue and green\n",
      "\n",
      "And never let my eye forget When our own breath gonne faint and fade\n",
      "\n",
      "That color is my friend Up the long stairway of the dead\n",
      "\n",
      "And purple must surround me too We shall remember well\n",
      "\n",
      "The blindness of the light, the whiteness\n",
      "\n",
      "The yellow of the sun is no more Of that white land. The footsteps, and\n",
      "\n",
      "Intrusive than the bluish snow The grasses in that light, and in the shade\n",
      "\n",
      "That falls on all of us. I must have The menacing life.\n",
      "\n",
      "Grey thoughts and blue thoughts walk with me\n",
      "\n",
      "If I am to go away at all. We played, too, once, in the warmest rooms.\n",
      "\n",
      "To one content, there is one content\n",
      "\n",
      "-------- Generated Poem 2 -------- For all mankind. The forest glades\n",
      "\n",
      "Are all the more for being in fable,\n",
      "\n",
      "There is a way in the world. And wood and town in legend shadowedThat way a man may live in a small town Once more to return. The one content\n",
      "\n",
      "His lifetime and see no man come, Has in it every happiness, more brave\n",
      "\n",
      "Must look on blue and green\n",
      "\n",
      "And never let my eye forget When our own breath gonne faint and fade\n",
      "\n",
      "That color is my friend Up the long stairway of the dead\n",
      "\n",
      "And purple must surround me too We shall remember well\n",
      "\n",
      "The blindness of the light, the whiteness\n",
      "\n",
      "The yellow of the sun is no more Of that white land. The footsteps, and\n",
      "\n",
      "Intrusive than the bluish snow The grasses in that light, and in the shade\n",
      "\n",
      "That falls on all of us. I must have The menacing life.\n",
      "\n",
      "Grey thoughts and blue thoughts walk with me\n",
      "\n",
      "If I am to go away at all. We played, too, once, in the warmest rooms.\n",
      "\n",
      "To one content, there is one content\n",
      "\n",
      "-------- Generated Poem 2 -------- For all mankind. The forest glades\n",
      "\n",
      "Are all the more for being in fable,\n",
      "\n",
      "There is a way in the world. And wood and town in legend shadowedThat way a man may live in a small town Once more to return. The one content\n",
      "\n",
      "His lifetime and see no man come, Has in it every happiness, more brave\n",
      "Human: Can you write a poem for me?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know.\n",
      "Exiting the conversation.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the chat model\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Create a memory object to store the conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Create a conversational retrieval chain by combining the chat model, retriever, and memory\n",
    "conversation = ConversationalRetrievalChain.from_llm(\n",
    "    llm, retriever=retriever, memory=memory, verbose=True\n",
    ")\n",
    "\n",
    "# Start an infinite loop to interact with the user\n",
    "# while(True):\n",
    "#     user_input = input(\"> \")  # Get user input\n",
    "#     result = conversation.invoke(user_input)  # Invoke the conversation chain with the user input\n",
    "#     print(result[\"answer\"])  # Print the chatbot's response\n",
    "\n",
    "\n",
    "# Start an infinite loop to interact with the user\n",
    "# Start an infinite loop to interact with the user\n",
    "while True:\n",
    "    user_input = input(\"> \")  # Get user input\n",
    "    if user_input.lower() == \"exit\":  # Check if the user wants to exit\n",
    "        print(\"Exiting the conversation.\")\n",
    "        break  # Exit the loop\n",
    "    result = conversation.invoke(user_input)  # Invoke the conversation chain with the user input\n",
    "    print(result[\"answer\"])  # Print the chatbot's response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 8 Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "PineconeConfigurationError",
     "evalue": "You haven't specified an Api-Key.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPineconeConfigurationError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/60/j13rv8sj6h7_s2bchmfjrwbm0000gn/T/ipykernel_34909/3487637395.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import necessary classes from pinecone, langchain_openai, and langchain_pinecone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpinecone\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPinecone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPinecone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the index name and initialize the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pinecone/control/pinecone.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, host, proxy_url, proxy_headers, ssl_ca_certs, ssl_verify, config, additional_headers, pool_threads, index_api, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             self.config = PineconeConfig.build(\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pinecone/config/pinecone_config.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(api_key, host, additional_headers, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ignoring PINECONE_ADDITIONAL_HEADERS: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         return ConfigBuilder.build(\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_headers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditional_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pinecone/config/config.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(api_key, host, proxy_url, proxy_headers, ssl_ca_certs, ssl_verify, additional_headers, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPineconeConfigurationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You haven't specified an Api-Key.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mPineconeConfigurationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You haven't specified a host.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPineconeConfigurationError\u001b[0m: You haven't specified an Api-Key."
     ]
    }
   ],
   "source": [
    "# Import necessary classes from pinecone, langchain_openai, and langchain_pinecone\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone()\n",
    "\n",
    "# Define the index name and initialize the index\n",
    "index_name = 'research-paper-rag-index'\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Describe the index statistics\n",
    "index.describe_index_stats()\n",
    "\n",
    "# Import OpenAI embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Import Pinecone vector store\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "# Import uuid for generating unique IDs\n",
    "from uuid import uuid4\n",
    "\n",
    "# Import document loader and text splitter\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load documents from the local directory\n",
    "loader = DirectoryLoader('./', glob=\"*.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents into smaller chunks for better processing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "document_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Generate unique IDs for each document chunk\n",
    "uuids = [str(uuid4()) for _ in range(len(document_chunks))]\n",
    "\n",
    "# Add document chunks to the vector store with the generated IDs\n",
    "vector_store.add_documents(documents=document_chunks, ids=uuids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
